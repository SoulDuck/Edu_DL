{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard 공간\n",
    "log_dir = \"../logs\"\n",
    "if os.path.exists(log_dir):\n",
    "    # 존재하면 이전 log 정보들을 지움 (섞이지 않기 위해)\n",
    "    shutil.rmtree(log_dir)\n",
    "else:\n",
    "    os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(init_op):\n",
    "    \"\"\"\n",
    "    initializer에 따른 초기학습에 대한 성능 비교\n",
    "    CIFAR를 통해 초기 학습에 얼마만큼의 차이를 보이는지 확인하고자 함    \n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    relu = lambda x: tf.nn.relu(x) # Activation Function\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name=\"X\")\n",
    "        y = tf.placeholder(tf.int64, shape=(None,), name='y')\n",
    "        phase_train = tf.placeholder_with_default(False, shape=(), name='phase_train')\n",
    "        \n",
    "        _reshaped_x = tf.reshape(X, (-1,32*32*3),name='flatten')\n",
    "        with tf.variable_scope('fc_block1'):\n",
    "            fc_layer = tf.layers.dense(_reshaped_x, 512, activation=relu,\n",
    "                                       kernel_initializer=init_op, name='dense')\n",
    "            dropout = tf.layers.dropout(fc_layer, 0.2,training=phase_train,name='dropout')\n",
    "\n",
    "        with tf.variable_scope('fc_block2'):\n",
    "            fc_layer = tf.layers.dense(dropout, 256, activation=relu,\n",
    "                                       kernel_initializer=init_op, name='dense')\n",
    "            dropout = tf.layers.dropout(fc_layer, 0.2,training=phase_train,name='dropout')\n",
    "\n",
    "        with tf.variable_scope('fc_block3'):\n",
    "            fc_layer = tf.layers.dense(dropout, 256, activation=relu,\n",
    "                                       kernel_initializer=init_op, name='dense')\n",
    "            dropout = tf.layers.dropout(fc_layer, 0.2,training=phase_train,name='dropout')\n",
    "\n",
    "        with tf.variable_scope('fc_block4'):\n",
    "            fc_layer = tf.layers.dense(dropout, 256, activation=relu,\n",
    "                                       kernel_initializer=init_op, name='dense')\n",
    "            dropout = tf.layers.dropout(fc_layer, 0.2, training=phase_train,name='dropout')\n",
    "\n",
    "        with tf.variable_scope('fc_block5'):\n",
    "            fc_layer = tf.layers.dense(dropout, 256, activation=relu,\n",
    "                                       kernel_initializer=init_op, name='dense')\n",
    "            dropout = tf.layers.dropout(fc_layer, 0.2, training=phase_train,name='dropout')\n",
    "            \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            logits = tf.layers.dense(dropout, 10, kernel_initializer=init_op, name='logit')\n",
    "\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "        with tf.variable_scope('eval'):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "            \n",
    "        global_step = tf.train.create_global_step()\n",
    "        training_op = (tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "                       .minimize(loss,global_step=global_step))\n",
    "\n",
    "    return graph\n",
    "\n",
    "def attach_summary(graph):\n",
    "    \"\"\"\n",
    "    Graph에 기록할 부분들을 summary operation에 추가\n",
    "    \"\"\"\n",
    "    # loss 추가\n",
    "    loss = graph.get_collection(tf.GraphKeys.LOSSES)[0]\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # accuracy 추가\n",
    "    accuracy = graph.get_tensor_by_name('eval/accuracy:0')\n",
    "    tf.summary.scalar('accuracy', accuracy)    \n",
    "    # Fully Connected layer의 weight 추가\n",
    "    weights = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'fc_block[1-5]/\\w+/kernel')\n",
    "    for idx, weight in enumerate(weights):\n",
    "        tf.summary.histogram('dense_layer{}'.format(idx), weight)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def cifar_generator(batch_size, data, labels):\n",
    "    '''\n",
    "    cifar 데이터셋을 배치 size 단위로 반환하는 generator\n",
    "\n",
    "    :param batch_size : 배치 크기\n",
    "    :param data : (Num,32,32,3)으로된 cifar 이미지\n",
    "    :param labels : (Num,)으로된 cifar 라벨\n",
    "    '''\n",
    "    start_idx = 0\n",
    "    num_step = len(data) // batch_size\n",
    "    indexes = np.arange(0, len(data))\n",
    "    while True:\n",
    "        if start_idx >= num_step-1:\n",
    "            np.random.shuffle(indexes)\n",
    "            start_idx = 0\n",
    "        else:\n",
    "            start_idx += 1            \n",
    "        batch_index = indexes[start_idx*batch_size:\n",
    "                              (start_idx+1)*batch_size]\n",
    "\n",
    "        batch_data = data[batch_index]\n",
    "        batch_label = labels[batch_index]\n",
    "        yield batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar_model(graph, save_dir, train_generator, test_x, test_y, n_epochs=20, batch_size=100):\n",
    "    \"\"\"\n",
    "    graph를 학습시키고, 그 경과를 tensorboard에서 볼 수 있도록 save_dir로 summary 저장\n",
    "    \n",
    "    \"\"\"\n",
    "    ##################\n",
    "    # Prepare Training\n",
    "    #     관련된 operation와 tensor들을 가져옴\n",
    "    ##################\n",
    "    # Input Tensor\n",
    "    X = graph.get_tensor_by_name('X:0')\n",
    "    y = graph.get_tensor_by_name('y:0')\n",
    "    phase_train = graph.get_tensor_by_name('phase_train:0')\n",
    "\n",
    "    # Train Operation\n",
    "    train_op = graph.get_collection(tf.GraphKeys.TRAIN_OP)[0]\n",
    "    global_step = graph.get_collection(tf.GraphKeys.GLOBAL_STEP)[0]\n",
    "\n",
    "    # Metric Operation\n",
    "    loss = graph.get_collection(tf.GraphKeys.LOSSES)[0]\n",
    "    accuracy = graph.get_tensor_by_name('eval/accuracy:0')\n",
    "    \n",
    "    # Summary Operation\n",
    "    summary_collection = graph.get_collection(tf.GraphKeys.SUMMARIES)\n",
    "    summary_op = tf.summary.merge(summary_collection)\n",
    "\n",
    "    # Summary Writer\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(save_dir,\"train\"), graph, flush_secs=5)\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(save_dir,\"test\"), flush_secs=5)\n",
    "\n",
    "    ###################\n",
    "    # Run Training\n",
    "    #     모델을 학습시키고, 그 경과를 기록\n",
    "    ###################\n",
    "    \n",
    "    console_format = \"[{}] acc : {:2.2f}% | loss : {:.3f}  \"\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # 변수초기화\n",
    "        init = [tf.local_variables_initializer(),\n",
    "                tf.global_variables_initializer()]\n",
    "        sess.run(init)\n",
    "        \n",
    "        t = tqdm(range(n_epochs))\n",
    "        for epoch in t:\n",
    "            # epoch이 시작할 때 test summary를 기록\n",
    "            summary, test_loss, test_acc = sess.run([summary_op,loss, accuracy], \n",
    "                                                    feed_dict={X:x_test,\n",
    "                                                               y:y_test})\n",
    "            test_writer.add_summary(summary, global_step=global_step.eval())\n",
    "            # print to console\n",
    "            t.set_description(console_format.format(\" test \", test_acc*100, test_loss))\n",
    "            t.refresh()\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                # model 학습\n",
    "                batch_x, batch_y = next(train_generator)\n",
    "                sess.run(train_op,\n",
    "                         feed_dict={X:batch_x,\n",
    "                                    y: batch_y,\n",
    "                                    phase_train:True})\n",
    "                \n",
    "                if step % 100 == 0:\n",
    "                    # 100 step 마다 train summary를 기록\n",
    "                    summary,train_loss, train_acc = sess.run([summary_op,loss, accuracy], \n",
    "                               feed_dict={X:batch_x,\n",
    "                                          y:batch_y})\n",
    "                    train_writer.add_summary(summary,global_step=global_step.eval())\n",
    "                    # print to console\n",
    "                    t.set_description(console_format.format(\" train\", train_acc*100, train_loss))\n",
    "                    t.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 데이터 셋 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training 관련 인자\n",
    "batch_size = 100 \n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cifar data\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# normalize Cifar Data (Cifar 데이터셋에 한에 Normalize)\n",
    "x_train = x_train/255.\n",
    "y_train = y_train.reshape(-1).astype(np.int64)\n",
    "x_test = x_test/255.\n",
    "y_test = y_test.reshape(-1).astype(np.int64)\n",
    "\n",
    "# create Generator\n",
    "train_generator = cifar_generator(batch_size,x_train, y_train)\n",
    "\n",
    "num_steps = len(x_train)//batch_size # epoch 별 step 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zero Initialization\n",
    "\n",
    "모든 Fully Connected Layer의 Weight들을 0으로 통일하였을 때, 어떤 식으로 학습되는지를 보고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.zeros()\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train Model\n",
    "save_dir = os.path.join(log_dir,\"zero-init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, x_test, y_test, n_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss가 늘 같은 값을 가진다. Relu의 특징 상 weight가 0 인경우 , gradient가 항상 0에 수렴하여, weight의 변화가 없기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constant Initialization\n",
    "\n",
    "모든 Fully Connected Layer의 Weight들을 0.01으로 통일하였을 때, 어떤 식으로 학습되는지를 보고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.constant(0.01)\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train Model\n",
    "save_dir = os.path.join(log_dir,\"constant-init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, x_test, y_test, n_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 weight가 같은 값으로 초기화된다면, graident가 모든 layer별 weight에게는 같이 적용되기 때문에, 사실 상 1개의 weight가 존재하는 것과 같은 효과를 가진다. 그래서 전형적인 underfitting의 그래프를 그린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normal distribution Initialization\n",
    "\n",
    "모든 Fully Connected Layer의 Weight들을 표준편차 0.01의 정규분포로 무작위하게 배치하였을 때, 초기 학습이 어떤식으로 동작하는지 보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표준편차가 0.01인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.random_normal(stddev=0.01)\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train Model\n",
    "save_dir = os.path.join(log_dir,\"normal-distribution_with_0.01_init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, x_test, y_test, n_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표준편차가 0.05인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.random_normal(stddev=0.05)\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train Model\n",
    "save_dir = os.path.join(log_dir,\"normal-distribution_with_0.05_init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, x_test, y_test, n_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표준편차가 0.1인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.random_normal(stddev=0.1)\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train Model\n",
    "save_dir = os.path.join(log_dir,\"normal-distribution_with_0.1_init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, x_test, y_test, n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Uniform Distribution Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최소 최대값이 0.01인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.random_uniform(minval=-0.01, \n",
    "                                         maxval=0.01)\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train model\n",
    "save_dir = os.path.join(log_dir,\"uniform-distribution-with-0.01-init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, \n",
    "                  x_test, y_test, n_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최소 최대값이 0.05인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.random_uniform(minval=-0.05, \n",
    "                                         maxval=0.05)\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train model\n",
    "save_dir = os.path.join(log_dir,\"uniform-distribution-with-0.05-init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, \n",
    "                  x_test, y_test, n_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최소 최대값이 0.1인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.random_uniform(minval=-0.1, \n",
    "                                         maxval=0.1)\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train model\n",
    "save_dir = os.path.join(log_dir,\"uniform-distribution-with-0.1-init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, \n",
    "                  x_test, y_test, n_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 적절한 표준편차를 정해주는 것은 어렵다.\n",
    "\n",
    "(현재 정확히 왜 문제가 되는지 근거가 부족합니다, 작았을 때와 컸을 때의 문제를 서술하면 좋습니다.)\n",
    "* 표준편차가 너무 크다면? -> <근거>\n",
    "* 표준편차가 너무 작다면? -> <근거> \n",
    "\n",
    "각 layer 별로 적절한 표준편차를 잡아나가야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. he Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.he_normal()\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train model\n",
    "save_dir = os.path.join(log_dir,\"he-init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, \n",
    "                  x_test, y_test, n_epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Glorot Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "init_op = tf.initializers.glorot_normal()\n",
    "graph = build_graph(init_op)\n",
    "graph = attach_summary(graph)\n",
    "\n",
    "# Train model\n",
    "save_dir = os.path.join(log_dir,\"glorot-init/\")\n",
    "train_cifar_model(graph, save_dir, train_generator, \n",
    "                  x_test, y_test, n_epochs, batch_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
