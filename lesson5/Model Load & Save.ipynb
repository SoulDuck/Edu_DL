{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가르쳐야 하는 것들에 대해서 정리해보자.\n",
    "\n",
    "1. Tensorflow Model을 어떻게 저장하고 불러올 것인가? \n",
    "   * 이걸 똑바로 이해하기 위해서는 Tensorflow가 어떤 식으로 이루어져 있는지 알아야 함.\n",
    "     왜 checkpoint와 meta_graph로 나뉘어져 있는지 이해하고, 각각 어떤 정보를 담고 있는지를 알아야 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow의 설계 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 transfer learning을 하기 전에, 먼저 텐서플로우의 구조부터 이해해보도록 하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "텐서플로우는 빌딩구조와 실행구조가 분리되어 있다.\n",
    "* 빌딩구조 : graph = tensors(edges) + operations(nodes)\n",
    "* 실행구조 : session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 빌딩구조\n",
    "\n",
    "graph는 nodes와 edges로 구성된 하나의 프로그램 모듈.\n",
    "\n",
    "* **nodes** : the units of computation\n",
    "* **edges** : the units of data that flow between operations\n",
    "\n",
    "\n",
    "![](../src/images/tensorflow 구조.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow의 graph는 node들의 집합이다. node들은 아래와 같은 요소들로 정의되어 있다.\n",
    "\n",
    "| 구성요소  | 간단 설명  | description | \n",
    "|---   |          ---|---|\n",
    "| name  | 고유한 지정 이름| unique identifier that's not used by any other nodes in the graph |\n",
    "| op    | 연산 종류(conv, lstm, add 등) | what operators to run (for example : `Add`, `MatMul`, `Conv2D`) |\n",
    "| input | 연산을 적용할 node의 name     | a list of strings, each one of which is the name of another node  |\n",
    "| device | 연산할 device의 종류  | defines where to run a node in a distributed environment, or when you want to force the operation onto CPU or GPU. |\n",
    "| attr | 연산의 속성값(conv filter 갯수 등) | {key : value} holding all the attributes of a node. the permanent properties of nodes, things that don't change at runtime |\n",
    "\n",
    "\n",
    "중요한 것 중 하나는 weights가 여기에 포함되어 있지 않다는 사실이다. 학습된 weights들은 어디에 포함되어 있을까? \n",
    "\n",
    "-> 바로 session(checkpoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 실행구조\n",
    "\n",
    "* fetch : 연산의 결과를 가져오는(fetch) 방법\n",
    "* feed :  placeholder에 값을 넣어 실행하는 방법\n",
    "\n",
    "``\n",
    "tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)\n",
    "``\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "세션 run의 구조\n",
    "![](../src/images/sess_run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 텐서플로우에서 값들을 저장되고 처리되는 단계는 실행구조(Session)에서 모두 이루어진다. \n",
    "\n",
    "graph는 이를 추상화시켜 놓은 것에 불과하다. \n",
    "\n",
    "그렇기 때문에 우리가 작업을 할 때는 변수를 초기화해주는 작업이 꼭 필요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(20, name='x')\n",
    "y = tf.Variable(10, name='y')\n",
    "z = tf.add(x,y)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 변수 초기화\n",
    "    # 변수를 초기화하지 않으면, \n",
    "    # FailedPreconditionError 발생\n",
    "    x.initializer.run() # x를 초기화\n",
    "    y.initializer.run() # y를 초기화\n",
    "\n",
    "    print(sess.run(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역으로 우리가 기존에 저장된 모델을 불러온다는 것은 어떤 식으로 이루어져야 할까?\n",
    "\n",
    "우선 우리는 불러올 모델에 대한 graph 구조가 필요하고, \n",
    "\n",
    "학습된 weights들의 값을 해당 graph로 할당해주어야 한다. \n",
    "\n",
    "학습된 weights들의 값을 해당 graph로 할당해주어야 하기 때문에, \n",
    "\n",
    "우리는 session에다가 주입을 해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단한 모델을 만들어서 학습을 시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-d0184ca2c727>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/ksj/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/ksj/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/ksj/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/ksj/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Load mnist dataset\n",
    "mnist = input_data.read_data_sets('./')\n",
    "\n",
    "n_inputs = mnist.train.images.shape[1]\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "graph = tf.get_default_graph() # 현재 default로 설정된 Tensorflow의 graph로 가져옴\n",
    "\n",
    "# input placeholder\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None,), name='y')\n",
    "\n",
    "leaky_relu = lambda x: tf.nn.leaky_relu(x) # Activation Function\n",
    "xavier_init = tf.initializers.variance_scaling() # Initialization for last fully-connected layer\n",
    "he_init = tf.initializers.variance_scaling(scale=2.) # Initialization for all layer except last layer\n",
    "\n",
    "with tf.variable_scope('cnn'):\n",
    "    # 28,28,1로 변경하여, convolution 연산이 가능하도록 함\n",
    "    _input = tf.reshape(X, (-1,28,28,1)) \n",
    "\n",
    "    with tf.variable_scope('conv_block1'):\n",
    "        conv_layer = tf.layers.conv2d(_input, 8, (3,3), \n",
    "                                        strides=(1,1),padding='same',\n",
    "                                        activation=leaky_relu, name='conv',\n",
    "                                        kernel_initializer=he_init)\n",
    "        max_pool = tf.layers.max_pooling2d(conv_layer,(2,2),(2,2),\n",
    "                                             name='pool', padding='same')\n",
    "\n",
    "    with tf.variable_scope('conv_block2'):\n",
    "        conv_layer = tf.layers.conv2d(max_pool, 8, (3,3), \n",
    "                                        strides=(1,1),padding='same',\n",
    "                                        activation=leaky_relu, name='conv',\n",
    "                                        kernel_initializer=he_init)\n",
    "        max_pool = tf.layers.max_pooling2d(conv_layer,(2,2),(2,2),\n",
    "                                             name='pool', padding='same')\n",
    "\n",
    "    with tf.variable_scope('conv_block3'):\n",
    "        conv_layer = tf.layers.conv2d(max_pool, 8, (3,3), \n",
    "                                        strides=(1,1),padding='same',\n",
    "                                        activation=leaky_relu, name='conv',\n",
    "                                        kernel_initializer=he_init)\n",
    "        max_pool = tf.layers.max_pooling2d(conv_layer,(2,2),(2,2),\n",
    "                                             name='pool', padding='same')\n",
    "\n",
    "    with tf.variable_scope('conv_block4'):\n",
    "        conv_layer = tf.layers.conv2d(max_pool, 8, (3,3), \n",
    "                                        strides=(1,1),padding='same',\n",
    "                                        activation=leaky_relu, name='conv',\n",
    "                                        kernel_initializer=he_init)\n",
    "        max_pool = tf.layers.max_pooling2d(conv_layer,(2,2),(2,2),\n",
    "                                             name='pool', padding='same')\n",
    "\n",
    "    with tf.variable_scope('conv_block5'):\n",
    "        conv_layer = tf.layers.conv2d(max_pool, 8, (3,3), \n",
    "                                        strides=(1,1),padding='same',\n",
    "                                        activation=leaky_relu, name='conv',\n",
    "                                        kernel_initializer=he_init)\n",
    "        max_pool = tf.layers.max_pooling2d(conv_layer,(2,2),(2,2),\n",
    "                                             name='pool', padding='same')\n",
    "\n",
    "    with tf.variable_scope(\"fully_connected\"):\n",
    "        avg_pool = tf.reduce_mean(max_pool, axis=[1,2],name=\"global_average_pooling\")\n",
    "        flatten = tf.layers.flatten(avg_pool,name='flatten')\n",
    "        dense_layer = tf.layers.dense(flatten, 32,\n",
    "                                      kernel_initializer=he_init,\n",
    "                                      activation=leaky_relu, \n",
    "                                      name='fc_layer')\n",
    "        logits = tf.layers.dense(dense_layer, n_outputs, \n",
    "                                 kernel_initializer=xavier_init,\n",
    "                                 name='output')\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        # sparse_softmax_cross_entropy_with_logits\n",
    "        # 소프트맥스 활성화 함수를 적용한 다음 크로스 엔트로피를 계산\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits)\n",
    "        # labels : 0~n_classes-1 사이의 정수로 된 label\n",
    "        # logits : 소프트맥스 활성화 함수로 들어가기 전의 네트워크 출력\n",
    "        loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "    with tf.variable_scope('eval'):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GLOBAL_VARIABLES\n",
    "> Key to collect Variable objects that are global (shared across machines). Default collection for all variables, except local ones.\n",
    "\n",
    "* LOCAL_VARIABLES\n",
    "> Key to collect local variables that are local to the machine and are not saved/restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch | Train accuracy: 0.28 Validation accuracy: 0.21\n",
      "0 epoch | Train accuracy: 0.28 Validation accuracy: 0.21\n",
      "1 epoch | Train accuracy: 0.39 Validation accuracy: 0.40\n",
      "1 epoch | Train accuracy: 0.39 Validation accuracy: 0.40\n",
      "2 epoch | Train accuracy: 0.52 Validation accuracy: 0.60\n",
      "2 epoch | Train accuracy: 0.52 Validation accuracy: 0.60\n",
      "3 epoch | Train accuracy: 0.70 Validation accuracy: 0.74\n",
      "3 epoch | Train accuracy: 0.70 Validation accuracy: 0.74\n",
      "4 epoch | Train accuracy: 0.86 Validation accuracy: 0.82\n",
      "4 epoch | Train accuracy: 0.86 Validation accuracy: 0.82\n",
      "5 epoch | Train accuracy: 0.84 Validation accuracy: 0.86\n",
      "5 epoch | Train accuracy: 0.84 Validation accuracy: 0.86\n",
      "6 epoch | Train accuracy: 0.91 Validation accuracy: 0.88\n",
      "6 epoch | Train accuracy: 0.91 Validation accuracy: 0.88\n",
      "7 epoch | Train accuracy: 0.81 Validation accuracy: 0.89\n",
      "7 epoch | Train accuracy: 0.81 Validation accuracy: 0.89\n",
      "8 epoch | Train accuracy: 0.91 Validation accuracy: 0.90\n",
      "8 epoch | Train accuracy: 0.91 Validation accuracy: 0.90\n",
      "9 epoch | Train accuracy: 0.88 Validation accuracy: 0.91\n",
      "9 epoch | Train accuracy: 0.88 Validation accuracy: 0.91\n",
      "10 epoch | Train accuracy: 0.95 Validation accuracy: 0.91\n",
      "10 epoch | Train accuracy: 0.95 Validation accuracy: 0.91\n",
      "11 epoch | Train accuracy: 0.89 Validation accuracy: 0.91\n",
      "11 epoch | Train accuracy: 0.89 Validation accuracy: 0.91\n",
      "12 epoch | Train accuracy: 0.94 Validation accuracy: 0.92\n",
      "12 epoch | Train accuracy: 0.94 Validation accuracy: 0.92\n",
      "13 epoch | Train accuracy: 0.95 Validation accuracy: 0.92\n",
      "13 epoch | Train accuracy: 0.95 Validation accuracy: 0.92\n",
      "14 epoch | Train accuracy: 0.95 Validation accuracy: 0.93\n",
      "14 epoch | Train accuracy: 0.95 Validation accuracy: 0.93\n",
      "15 epoch | Train accuracy: 0.95 Validation accuracy: 0.93\n",
      "15 epoch | Train accuracy: 0.95 Validation accuracy: 0.93\n",
      "16 epoch | Train accuracy: 0.92 Validation accuracy: 0.93\n",
      "16 epoch | Train accuracy: 0.92 Validation accuracy: 0.93\n",
      "17 epoch | Train accuracy: 0.95 Validation accuracy: 0.93\n",
      "17 epoch | Train accuracy: 0.95 Validation accuracy: 0.93\n",
      "18 epoch | Train accuracy: 0.97 Validation accuracy: 0.93\n",
      "18 epoch | Train accuracy: 0.97 Validation accuracy: 0.93\n",
      "19 epoch | Train accuracy: 0.94 Validation accuracy: 0.94\n",
      "19 epoch | Train accuracy: 0.94 Validation accuracy: 0.94\n",
      ": 100%|██████████| 20/20 [06:38<00:00, 19.93s/it]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# 보통은 global_variables_initializer만 존재해도 상관없으나\n",
    "# 간혹 안될 경우가 있어서, tf.local_variables_initializer를 추가\n",
    "init = [tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer()]\n",
    "sess.run(init)\n",
    "epoch_bar = tqdm(range(n_epochs))\n",
    "for epoch in epoch_bar:\n",
    "    for iteration in range(mnist.train.num_examples // batch_size):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        sess.run(training_op, feed_dict={X: X_batch,y: y_batch})\n",
    "\n",
    "    acc_train = sess.run(accuracy,feed_dict={X: X_batch,y: y_batch})\n",
    "    acc_val = sess.run(accuracy,feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "    epoch_bar.set_description(\n",
    "        \"{} epoch | Train accuracy: {:2.2f} Validation accuracy: {:2.2f}\\n\".format(epoch, acc_train, acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variables = {}\n",
    "with graph.as_default():\n",
    "    # 학습된 모든 variable들은 train_variables에 담겨져 있음\n",
    "    # train_variables 순회하면서 가져올 수 있다.\n",
    "    \n",
    "    #train_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"cnn/conv_block[1-5]/conv/kernel\")\n",
    "    train_variables = tf.global_variables()\n",
    "    for variable in train_variables:\n",
    "        tensor_name, tensor_index = variable.name.split(\":\")\n",
    "        # 저장된 weights 가져오기\n",
    "        weights = variable.read_value().eval(session=sess)\n",
    "        model_variables[tensor_name] = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.f)\n",
    "\n",
    "모든 변수들은 자동적으로 그들이 만들어진 graph에 쌓입니다. \n",
    "\n",
    "기본적으로, 생성자는 그래프 컬렉션(graph collection) GraphKeys.VARIABLES에 변수를 추가합니다. \n",
    "\n",
    "편의 함수인 all_variables()은 컬렉션의 내용을 반환합니다.\n",
    "\n",
    "\n",
    "머신 러닝 모델을 만들 때, 학습 가능한 모델 매개변수를 가지고 있는 변수와 global step 변수과 같이 학습 단계를 계산하기 위한 다른 변수로 구분하는 것은 종종 편리합니다. \n",
    "\n",
    "이것을 더 쉽게 하기위해, 변수 생성자는 trainable=<bool> 매개변수를 지원합니다. \n",
    "    \n",
    "True일 때 새로운 변수는 그래프 컬렉션 GraphKeys.TRAINABLE_VARIABLES에 추가됩니다. \n",
    "\n",
    "편의 함수 trainable_variables()는 이 컬렉션의 내용을 반환합니다. \n",
    "\n",
    "다양한 Optimizer 클래스는 이 컬렉션을 최적화(optimize) 변수의 기본 리스트로 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 모델을 저장하고 복원하기 위해서는 두 가지(graph, variables)를 저장하고 복원할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph Save & Load\n",
    "\n",
    "그래프는 기본적으로 `tf.train.export_meta_graph`와 `tf.train.import_meta_graph`로 관리된다. Tensorflow는 graph를 저장할 때 기본적으로 `MetaGraph`로 저장된다. graph를 관리하는 데에는 단순히 Graph의 node와 edge 뿐만 아니라 그 외 실행환경에 대한 정보도 같이 있어야 한다고 판단하기 때문이다.\n",
    "\n",
    "그렇기 때문에 Tensorflow의 버전 정보, 저장한 Saver에 대한 정보, 그외 CollectionDef에 대한 정보등이 있다. MetaGraph가 있으면 나중에는 처음부터 모델을 구축하지 않고 가져와서 계속 훈련도 가능하다\n",
    "\n",
    "MetaGraph에 있는 것\n",
    "\n",
    "| 종류 | 설명 |\n",
    "|----|------|\n",
    "|GraphDef| 그래프를 묘사 |\n",
    "|MetaInfoDef| 버전과 기타 사용자 정보 같은 메타 정보를 포함  |\n",
    "|SaverDef| saver에 대한 정보 |\n",
    "|CollectionDef| `Variables`, `QueueRunners`와 같은 추가적인 요소를 더 자세히 설명 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metagraph를 Import할 때는 implicit하게 현재 default graph로 import된다.\n",
    "\n",
    "\n",
    "참고자료 \n",
    "* [텐서플로우 한글 문서](https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/how_tos/meta_graph/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save graph\n",
    "save_dir = \"./saves/\"\n",
    "graph_path = os.path.join(save_dir,\"graph.meta\")\n",
    "\n",
    "tf.train.export_meta_graph(graph_path, graph=graph);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before import_meta_graph : \n",
      " []\n",
      "After import_meta_graph : \n",
      " [<tf.Operation 'X' type=Placeholder>, <tf.Operation 'y' type=Placeholder>, <tf.Operation 'cnn/Reshape/shape' type=Const>]\n"
     ]
    }
   ],
   "source": [
    "# load graph\n",
    "graph2 = tf.Graph()\n",
    "print(\"before import_meta_graph : \\n\",graph2.get_operations()) # 비어있음\n",
    "\n",
    "with graph2.as_default():\n",
    "    meta_graph = tf.train.import_meta_graph(graph_path)\n",
    "    \n",
    "print(\"After import_meta_graph : \\n\",graph2.get_operations()[:3]) # 차있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. variable Save & Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 Variable에 대한 관리는 Saver가 담당한다. Variable은 학습에 따라 계속 변하는 Time-variant한 값이기 때문에, 기본적으로는 `checkpoints` 개념으로 저장되고 로드된다. 아래는 공식문서에서 가져온 Saver에 대한 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Saver\n",
    "----\n",
    "\n",
    "Saves and restores variables.\n",
    "\n",
    "See Variables for an overview of variables, saving and restoring.\n",
    "\n",
    "The Saver class adds ops to save and restore variables to and from checkpoints. It also provides convenience methods to run these ops.\n",
    "\n",
    "Checkpoints are binary files in a proprietary format which map variable names to tensor values. The best way to examine the contents of a checkpoint is to load it using a Saver.\n",
    "\n",
    "\n",
    "saving할 때 나오는 것들은 아래와 같다.\n",
    "\n",
    "| 확장자 | 설명 |\n",
    "|---|---|\n",
    "| .meta  | containing the graph structure |\n",
    "| .data  | containing the values of variables |\n",
    "| .index | identifying the checkpoint |\n",
    "| .checkpoint | a protocol buffer with a list of recent checkpoints |\n",
    "\n",
    "핵심은 .meta 파일과 .checkpoint 파일로, 이 두 파일이 있으면 이전 Session으로 복구가 가능하다 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./saves/model'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Variable(Session)\n",
    "save_dir = \"./saves/\" # 저장할 디렉토리\n",
    "os.makedirs(save_dir,exist_ok=True) \n",
    "save_path = os.path.join(save_dir,\"model\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver.save(sess, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saves/model\n"
     ]
    }
   ],
   "source": [
    "# Load Variable(Session)\n",
    "with tf.Graph().as_default() as restored_graph:\n",
    "    meta_graph = tf.train.import_meta_graph(graph_path)\n",
    "    restored_sess = tf.Session(graph=restored_graph)\n",
    "    meta_graph.restore(restored_sess, save_path=save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}